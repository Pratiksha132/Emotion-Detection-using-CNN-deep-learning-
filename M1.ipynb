{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10356\\210559016.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msignal\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal as signal\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_jpg_files(folder_path):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter out jpg files\n",
    "    jpg_files = [file for file in files if file.endswith('.jpg')]\n",
    "    \n",
    "    images = []\n",
    "    for jpg_file in jpg_files:\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(folder_path, jpg_file)\n",
    "        \n",
    "        # Open and read the image\n",
    "        image = Image.open(file_path)\n",
    "        images.append(image)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Example usage\n",
    "folder_path =\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Pogramming\\\\Arousal\\\\High\"\n",
    "images = read_jpg_files(folder_path)\n",
    "\n",
    "# Display the first image\n",
    "if images:\n",
    "    images[0].show()\n",
    "else:\n",
    "    print(\"No images found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ecg(ecg_signal, fs=128):\n",
    "    # Bandpass filter the ECG signal\n",
    "    nyquist = 0.5 * fs\n",
    "    low = 0.5 / nyquist\n",
    "    high = 15 / nyquist\n",
    "    b, a = butter(1, [low, high], btype='band')\n",
    "    filtered_ecg = filtfilt(b, a, ecg_signal)\n",
    "    \n",
    "    # Pan-Tompkins QRS detection algorithm (simplified version)\n",
    "    diff = np.diff(filtered_ecg)\n",
    "    squared = diff ** 2\n",
    "    integrated = np.convolve(squared, np.ones(15) / 15)\n",
    "    peaks, _ = signal.find_peaks(integrated, distance=fs//2)  # Assuming a distance between peaks\n",
    "    \n",
    "    # Calculate RR intervals\n",
    "    rr_intervals = np.diff(peaks) / fs  # Convert to seconds\n",
    "    \n",
    "    return rr_intervals\n",
    "\n",
    "def preprocess_gsr(gsr_signal, fs=128):\n",
    "    # Bandpass filter the GSR signal\n",
    "    nyquist = 0.5 * fs\n",
    "    low = 0.05 / nyquist\n",
    "    high = 19 / nyquist\n",
    "    b, a = butter(1, [low, high], btype='band')\n",
    "    filtered_gsr = filtfilt(b, a, gsr_signal)\n",
    "    \n",
    "    # Resample the signal\n",
    "    resampled_gsr = signal.resample(filtered_gsr, int(len(filtered_gsr) * 10 / fs))  # Resample to 10 Hz\n",
    "    \n",
    "    # SCR peak detection (simplified version)\n",
    "    peaks, _ = signal.find_peaks(resampled_gsr, distance=20)  # Assuming a minimum distance between peaks\n",
    "    \n",
    "    return resampled_gsr, peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(rr_intervals, gsr_peaks, resampled_gsr):\n",
    "    # ECG features\n",
    "    ecg_features = {\n",
    "        'mean_rr': np.mean(rr_intervals),\n",
    "        'std_rr': np.std(rr_intervals),\n",
    "        'rmssd': np.sqrt(np.mean(np.square(np.diff(rr_intervals))))\n",
    "    }\n",
    "    \n",
    "    # GSR features\n",
    "    gsr_amplitudes = resampled_gsr[gsr_peaks]\n",
    "    gsr_features = {\n",
    "        'mean_amplitude': np.mean(gsr_amplitudes),\n",
    "        'std_amplitude': np.std(gsr_amplitudes),\n",
    "        'num_peaks': len(gsr_peaks)\n",
    "    }\n",
    "    \n",
    "    # Combine features\n",
    "    features = {**ecg_features, **gsr_features}\n",
    "    return features\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'ecg_signal': [np.random.rand(1000), np.random.rand(1000)],\n",
    "    'gsr_signal': [np.random.rand(1000), np.random.rand(1000)],\n",
    "    'emotion': ['happy', 'sad']\n",
    "})\n",
    "# Preprocess dataset and extract features\n",
    "features_list = []\n",
    "labels = data['emotion']  # Assuming 'emotion' is the target column\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    ecg_signal = np.array(row['ecg_signal'])  # Assuming ECG signal is stored as an array\n",
    "    gsr_signal = np.array(row['gsr_signal'])  # Assuming GSR signal is stored as an array\n",
    "    \n",
    "    rr_intervals = preprocess_ecg(ecg_signal)\n",
    "    resampled_gsr, gsr_peaks = preprocess_gsr(gsr_signal)\n",
    "    \n",
    "    features = extract_features(rr_intervals, gsr_peaks, resampled_gsr)\n",
    "    features_list.append(features)\n",
    "\n",
    "# Create a DataFrame from the features\n",
    "features_df = pd.DataFrame(features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     mean_rr    std_rr     rmssd  mean_amplitude  std_amplitude  num_peaks  \\\n",
      "0   0.374540  0.031429  0.642032        0.051682       0.103124          5   \n",
      "1   0.950714  0.636410  0.084140        0.531355       0.902553          0   \n",
      "2   0.731994  0.314356  0.161629        0.540635       0.505252          8   \n",
      "3   0.598658  0.508571  0.898554        0.637430       0.826457          0   \n",
      "4   0.156019  0.907566  0.606429        0.726091       0.320050          4   \n",
      "..       ...       ...       ...             ...            ...        ...   \n",
      "95  0.493796  0.349210  0.522243        0.930757       0.353352          2   \n",
      "96  0.522733  0.725956  0.769994        0.858413       0.583656          0   \n",
      "97  0.427541  0.897110  0.215821        0.428994       0.077735          4   \n",
      "98  0.025419  0.887086  0.622890        0.750871       0.974395          3   \n",
      "99  0.107891  0.779876  0.085347        0.754543       0.986211          9   \n",
      "\n",
      "    emotion  \n",
      "0         1  \n",
      "1         0  \n",
      "2         1  \n",
      "3         0  \n",
      "4         1  \n",
      "..      ...  \n",
      "95        1  \n",
      "96        0  \n",
      "97        1  \n",
      "98        1  \n",
      "99        1  \n",
      "\n",
      "[100 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_rr</th>\n",
       "      <th>std_rr</th>\n",
       "      <th>rmssd</th>\n",
       "      <th>mean_amplitude</th>\n",
       "      <th>std_amplitude</th>\n",
       "      <th>num_peaks</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.470181</td>\n",
       "      <td>0.497832</td>\n",
       "      <td>0.517601</td>\n",
       "      <td>0.491149</td>\n",
       "      <td>0.516046</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.297489</td>\n",
       "      <td>0.293111</td>\n",
       "      <td>0.293426</td>\n",
       "      <td>0.293452</td>\n",
       "      <td>0.318601</td>\n",
       "      <td>2.900279</td>\n",
       "      <td>0.502519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.005522</td>\n",
       "      <td>0.006952</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>0.014393</td>\n",
       "      <td>0.010838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.193201</td>\n",
       "      <td>0.242005</td>\n",
       "      <td>0.276880</td>\n",
       "      <td>0.249615</td>\n",
       "      <td>0.263935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.464142</td>\n",
       "      <td>0.505625</td>\n",
       "      <td>0.562555</td>\n",
       "      <td>0.509718</td>\n",
       "      <td>0.525399</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.730203</td>\n",
       "      <td>0.766184</td>\n",
       "      <td>0.752367</td>\n",
       "      <td>0.735778</td>\n",
       "      <td>0.796884</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.986887</td>\n",
       "      <td>0.985650</td>\n",
       "      <td>0.990054</td>\n",
       "      <td>0.990505</td>\n",
       "      <td>0.992965</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean_rr      std_rr       rmssd  mean_amplitude  std_amplitude  \\\n",
       "count  100.000000  100.000000  100.000000      100.000000     100.000000   \n",
       "mean     0.470181    0.497832    0.517601        0.491149       0.516046   \n",
       "std      0.297489    0.293111    0.293426        0.293452       0.318601   \n",
       "min      0.005522    0.006952    0.005062        0.014393       0.010838   \n",
       "25%      0.193201    0.242005    0.276880        0.249615       0.263935   \n",
       "50%      0.464142    0.505625    0.562555        0.509718       0.525399   \n",
       "75%      0.730203    0.766184    0.752367        0.735778       0.796884   \n",
       "max      0.986887    0.985650    0.990054        0.990505       0.992965   \n",
       "\n",
       "        num_peaks     emotion  \n",
       "count  100.000000  100.000000  \n",
       "mean     3.750000    0.500000  \n",
       "std      2.900279    0.502519  \n",
       "min      0.000000    0.000000  \n",
       "25%      1.000000    0.000000  \n",
       "50%      4.000000    0.500000  \n",
       "75%      6.000000    1.000000  \n",
       "max      9.000000    1.000000  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 151ms/step - loss: 0.6970 - accuracy: 0.5125 - val_loss: 0.6984 - val_accuracy: 0.3500\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6841 - accuracy: 0.6250 - val_loss: 0.6995 - val_accuracy: 0.4500\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6731 - accuracy: 0.5750 - val_loss: 0.6997 - val_accuracy: 0.4500\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6661 - accuracy: 0.6500 - val_loss: 0.7020 - val_accuracy: 0.4000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6595 - accuracy: 0.7000 - val_loss: 0.7050 - val_accuracy: 0.4500\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6551 - accuracy: 0.6625 - val_loss: 0.7087 - val_accuracy: 0.4500\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6491 - accuracy: 0.6875 - val_loss: 0.7117 - val_accuracy: 0.4500\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6450 - accuracy: 0.7000 - val_loss: 0.7158 - val_accuracy: 0.4500\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6414 - accuracy: 0.6750 - val_loss: 0.7192 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6373 - accuracy: 0.6750 - val_loss: 0.7254 - val_accuracy: 0.4500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7254 - accuracy: 0.4500\n",
      "Test Accuracy: 0.4500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming features_df is the DataFrame with the features\n",
    "# Mockup data for demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "features_df = pd.DataFrame({\n",
    "    'mean_rr': np.random.rand(n_samples),\n",
    "    'std_rr': np.random.rand(n_samples),\n",
    "    'rmssd': np.random.rand(n_samples),\n",
    "    'mean_amplitude': np.random.rand(n_samples),\n",
    "    'std_amplitude': np.random.rand(n_samples),\n",
    "    'num_peaks': np.random.randint(0, 10, n_samples),\n",
    "    'emotion': np.random.randint(0, 2, n_samples)  # Binary classification\n",
    "})\n",
    "\n",
    "# Splitting data into features and labels\n",
    "X = features_df.drop('emotion', axis=1).values\n",
    "y = features_df['emotion'].values\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input data to fit Conv1D layer (samples, time steps, features)\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "def build_dcnn(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = build_dcnn(input_shape)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
